{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6796277-39d4-492b-b589-7800f2447219",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    !pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import flax\n",
    "except ImportError:\n",
    "    !pip install -q flax\n",
    "    \n",
    "try:\n",
    "    import optax\n",
    "except ImportError:\n",
    "    !pip install -q optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4736d-09ad-4613-836a-be9477f2a12d",
   "metadata": {},
   "source": [
    "# Using flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae866c-44de-44e0-86b5-bae0aba20441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "t = np.sort(\n",
    "    np.append(\n",
    "        np.random.uniform(0, 3.8, 28),\n",
    "        np.random.uniform(5.5, 10, 18),\n",
    "    )\n",
    ")\n",
    "yerr = np.random.uniform(0.08, 0.22, len(t))\n",
    "y = (\n",
    "    0.2 * (t - 5)\n",
    "    + np.sin(3 * t + 0.1 * (t - 5) ** 2)\n",
    "    + yerr * np.random.randn(len(t))\n",
    ")\n",
    "\n",
    "true_t = np.linspace(0, 10, 100)\n",
    "true_y = 0.2 * (true_t - 5) + np.sin(3 * true_t + 0.1 * (true_t - 5) ** 2)\n",
    "\n",
    "plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3)\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y [ppm]\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "_ = plt.title(\"simulated data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c0c69-36e3-4f11-a1ab-2477286742ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen.initializers import zeros\n",
    "\n",
    "import optax\n",
    "\n",
    "from tinygp import kernels, GaussianProcess\n",
    "\n",
    "\n",
    "class GPModule(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, yerr, y, t):\n",
    "        mean = self.param(\"mean\", zeros, ())\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "\n",
    "        log_sigma1 = self.param(\"log_sigma1\", zeros, ())\n",
    "        log_rho1 = self.param(\"log_rho1\", zeros, ())\n",
    "        log_tau = self.param(\"log_tau\", zeros, ())\n",
    "        kernel1 = (\n",
    "            jnp.exp(2 * log_sigma1)\n",
    "            * kernels.ExpSquared(jnp.exp(log_tau))\n",
    "            * kernels.Cosine(jnp.exp(log_rho1))\n",
    "        )\n",
    "\n",
    "        log_sigma2 = self.param(\"log_sigma2\", zeros, ())\n",
    "        log_rho2 = self.param(\"log_rho2\", zeros, ())\n",
    "        kernel2 = jnp.exp(2 * log_sigma2) * kernels.Matern32(jnp.exp(log_rho2))\n",
    "\n",
    "        kernel = kernel1 + kernel2\n",
    "        gp = GaussianProcess(\n",
    "            kernel, x, diag=yerr ** 2 + jnp.exp(log_jitter), mean=mean\n",
    "        )\n",
    "\n",
    "        loss = -gp.condition(y)\n",
    "        pred = gp.predict(y, t)\n",
    "\n",
    "        return loss, pred\n",
    "\n",
    "\n",
    "model = GPModule()\n",
    "\n",
    "\n",
    "def loss(params):\n",
    "    return model.apply(params, t, yerr, y, true_t)[0]\n",
    "\n",
    "\n",
    "params = model.init(jax.random.PRNGKey(0), t, yerr, y, true_t)\n",
    "tx = optax.sgd(learning_rate=3e-3)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "\n",
    "losses = []\n",
    "for i in range(1001):\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    losses.append(loss_val)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss step {}: \".format(i), loss_val)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"negative log likelihood\")\n",
    "_ = plt.xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d6133-0fb1-43bc-98af-8221037e5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.apply(params, t, yerr, y, true_t)[1]\n",
    "\n",
    "plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3, label=\"truth\")\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(true_t, pred, label=\"max. like. model\")\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y [ppm]\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.legend()\n",
    "_ = plt.title(\"simulated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda214d-ce77-4e2a-99e3-5134f23ec0e6",
   "metadata": {},
   "source": [
    "# Deep kernel learning with flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd3917-a738-47d6-841f-e0bfb9e68fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random = np.random.default_rng(567)\n",
    "\n",
    "noise = 0.1\n",
    "\n",
    "x = np.sort(random.uniform(-1, 1, 200))\n",
    "y = 2 * (x > 0) - 1 + random.normal(0.0, noise, len(x))\n",
    "t = np.linspace(-1.5, 1.5, 500)\n",
    "\n",
    "plt.plot(x, y, \".k\")\n",
    "plt.plot(t, 2 * (t > 0) - 1)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.3, 1.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0065dea-379a-4e0d-8cf2-f460c8126a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen.initializers import zeros\n",
    "\n",
    "from tinygp import kernels, GaussianProcess\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=100)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=20)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPLoss(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, t):\n",
    "        extr = FeatureExtractor()\n",
    "        x = extr(x[:, None])\n",
    "        t = extr(t[:, None])\n",
    "\n",
    "        xmin = jnp.min(x, axis=0, keepdims=True)\n",
    "        xmax = jnp.max(x, axis=0, keepdims=True)\n",
    "        x = (x - xmin) / (xmax - xmin)\n",
    "        t = (t - xmin) / (xmax - xmin)\n",
    "\n",
    "        mean = self.param(\"mean\", zeros, ())\n",
    "        log_sigma = self.param(\"log_sigma\", zeros, ())\n",
    "        log_rho = self.param(\"log_rho\", zeros, (x.shape[1],))\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "        kernel = jnp.exp(2 * log_sigma) * kernels.Matern32(\n",
    "            jnp.exp(2 * log_rho)\n",
    "        )\n",
    "\n",
    "        gp = GaussianProcess(\n",
    "            kernel, x, diag=noise ** 2 + jnp.exp(2 * log_jitter), mean=mean\n",
    "        )\n",
    "        return -gp.condition(y), gp.predict(y, t, return_var=True), (x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c94ac-27f8-4fe7-819f-f3140e9be5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "model = GPLoss()\n",
    "\n",
    "\n",
    "def loss(params):\n",
    "    return model.apply(params, x, y, t)[0]\n",
    "\n",
    "\n",
    "params = model.init(jax.random.PRNGKey(0), x, y, t)\n",
    "tx = optax.sgd(learning_rate=1e-4)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "\n",
    "for i in range(1001):\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss step {}: \".format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec45d8c-7db5-442d-b933-7193c7903167",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = model.apply(params, x, y, t)[1]\n",
    "plt.plot(x, y, \".k\")\n",
    "plt.plot(t, mu)\n",
    "plt.fill_between(t, mu + np.sqrt(var), mu - np.sqrt(var), alpha=0.5)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.3, 1.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ab7af-9766-499d-b353-84f2a6c1ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp, tp = model.apply(params, x, y, t)[2]\n",
    "\n",
    "plt.plot(t, tp)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"warped x\")\n",
    "plt.xlim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281b035-513a-4215-87fd-1a83b52ebd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
